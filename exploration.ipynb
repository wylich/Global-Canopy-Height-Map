{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for data exploration and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"datasets_pytorch/ai4forest_camera/train.csv\", )\n",
    "df_val = pd.read_csv(\"datasets_pytorch/ai4forest_camera/val.csv\")\n",
    "df_fix = pd.read_csv(\"datasets_pytorch/ai4forest_camera/fix_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of 30m pixels in the training set\n",
    "g30 = df_train[\"g30\"].sum()\n",
    "g25 = df_train[\"g25\"].sum()\n",
    "g20 = df_train[\"g20\"].sum()\n",
    "g15 = df_train[\"g15\"].sum()\n",
    "g10 = df_train[\"g10\"].sum()\n",
    "g5 = df_train[\"g5\"].sum()\n",
    "gsum = g30 + g25 + g20 + g15 + g10 + g5\n",
    "g30/gsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(df):    \n",
    "    #change hemisphere from categorical to binary\n",
    "    df['hemisphere'] = df['hemisphere'].astype('category').cat.codes\n",
    "    #rename hemisphere to hemi_S\n",
    "    df.rename(columns={'hemisphere':'hemi_S'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "df_train = cleanup(df_train)\n",
    "df_val = cleanup(df_val)\n",
    "df_fix = cleanup(df_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets_pytorch/ai4forest_camera/samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_eu = df[\n",
    "#         (df['lat']>35)  & (-12<df['long']) & (df['long']<60) \n",
    "#         ]\n",
    "# df_eu.to_csv(\"datasets_pytorch/ai4forest_camera/samples_eu.csv\")\n",
    "# len(df_eu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = fig.add_subplot(111)\n",
    "#scatter plot with marker color depending on df_train['hemisphere']\n",
    "ax.scatter(df_val['long'], df_val['lat'], c=\"black\", alpha=0.7, s=1)\n",
    "ax.scatter(df_train['long'], df_train['lat'], c=\"lightgrey\", alpha=0.7, s=1)\n",
    "# ax.scatter(df_fix['long'], df_fix['lat'], c=\"black\", alpha=1, s=1)\n",
    "# ax.scatter(lon, lat, c=data['zone_number'], cmap='prism', alpha=0.5, s=12) # time zones\n",
    "# ax.set_xlabel('Longitude')\n",
    "# ax.set_ylabel('Latitude')\n",
    "plt.xlim(-185,185)\n",
    "plt.xticks(np.arange(-180, 181, 30))\n",
    "plt.ylim(-90,90)\n",
    "plt.yticks(np.arange(-90, 91, 30))\n",
    "plt.show()\n",
    "\n",
    "# overlay world map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_idx = df_fix #.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the file\n",
    "# npz_name = df_fix_idx['path'][2]\n",
    "npz_name = df_train['path'][0]\n",
    "# npz_name = \"sentinel_1_2_worldwide_utm_1N_25-0000012544-0000012544_0.npz\"\n",
    "\n",
    "# npz = np.load(os.path.join(\"data\\samples.zip\\samples\",npz_name))\n",
    "\n",
    "npz = np.load(os.path.join(\"datasets_pytorch/ai4forest_camera/data\" ,npz_name))\n",
    "\n",
    "# Check contents\n",
    "print(\"Contents:\", npz.files)\n",
    "print(\"Type npz: \", type(npz))\n",
    "\n",
    "# Access an image\n",
    "arr = npz['data']\n",
    "print(\"Array shape:\", arr.shape)\n",
    "print(\"Data struct type:\", type(arr))\n",
    "print(\"Data type:\", arr.dtype)\n",
    "\n",
    "# in array shape for each of the 14 channels\n",
    "# Load the NPZ file\n",
    "# Access the data array\n",
    "arr = npz['data']\n",
    "print(\"Array shape:\", arr.shape)  # Should be (14, 512, 512)\n",
    "\n",
    "# Create a figure with 14 subplots (adjust layout as needed)\n",
    "fig, axes = plt.subplots(nrows=7, ncols=2, figsize=(15, 30))  # 7 rows, 2 columns\n",
    "axes = axes.ravel()  # Flatten the axes array for easy iteration\n",
    "\n",
    "# Plot each channel\n",
    "for i in range(14):\n",
    "    # Display the channel\n",
    "    im = axes[i].imshow(arr[i], cmap='viridis')\n",
    "    axes[i].set_title(f'Channel {i+1}')\n",
    "    fig.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Close the NPZ file\n",
    "npz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load(os.path.join(\"datasets_pytorch/ai4forest_camera/data\" ,npz_name))\n",
    "\n",
    "# Check contents\n",
    "print(npz['labels'].shape)\n",
    "\n",
    "# Create a figure with 14 subplots (adjust layout as needed)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))  # 7 rows, 2 columns\n",
    "axes = axes.ravel()  # Flatten the axes array for easy iteration\n",
    "\n",
    "# Plot each channel\n",
    "for i in range(3):\n",
    "    # Display the channel\n",
    "    im = axes[i].imshow(npz['labels'][i], cmap='viridis')\n",
    "    axes[i].set_title(f'Label {i+1}')\n",
    "    fig.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the file\n",
    "npz_name = df_fix_idx['path'][8]\n",
    "# npz_name = \"sentinel_1_2_worldwide_utm_1N_25-0000012544-0000012544_0.npz\"\n",
    "# npz = np.load(os.path.join(\"data\\samples.zip\\samples\",npz_name))\n",
    "\n",
    "npz_arr = []\n",
    "for name in df_fix_idx['path']:\n",
    "    npz_arr.append(name)\n",
    "\n",
    "# in array shape for each of the 14 channels\n",
    "# Load the NPZ file\n",
    "# Access the data array\n",
    "# arr = npz['data']\n",
    "# print(\"Array shape:\", arr.shape)  # Should be (14, 512, 512)\n",
    "\n",
    "# Create a figure with 14 subplots (adjust layout as needed)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(30, 10))  # 7 rows, 2 columns\n",
    "axes = axes.ravel()  # Flatten the axes array for easy iteration\n",
    "\n",
    "for i, name in enumerate(npz_arr):\n",
    "    arr = np.load(os.path.join(\"datasets_pytorch/ai4forest_camera/data\", name))\n",
    "    im = axes[i].imshow(arr['data'][6], cmap='viridis')\n",
    "    axes[i].set_title(f'Img {i}')\n",
    "    # fig.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Close the NPZ file\n",
    "npz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = pd.read_csv('wandb_export_models.csv')\n",
    "# drop all columns starting with \"fixval\"\n",
    "df_models.drop(df_models.filter(regex=\"fixval\"), axis=1, inplace=True)\n",
    "df_models.drop(df_models.filter(regex=\"train/l\"), axis=1, inplace=True)\n",
    "df_models.drop(df_models.filter(regex=\"train/shift_l\"), axis=1, inplace=True)\n",
    "drop_cols = ['sampler', 'train/ips_throughput', 'optim','phase_runtime', 'learning_rate', \n",
    "             'num_workers_per_gpu','use_weighting_quantile', 'Created', 'n_lr_cycles', 'cyclic_mode', \n",
    "             'use_mixup', 'fp16', 'Notes', 'wandb', 'loss_name', 'arch', 'iteration', 'profiler', 'Tags', \n",
    "             'optimizer', 'weight_decay', 'dataset', 'initial_lr', 'use_amp', 'model_path', 'Sweep', \n",
    "             'device', 'use_input_clipping', 'model', 'seed', 'use_grad_clipping', 'use_label_rescaling', \n",
    "             'State', 'n_iterations', 'User', 'computer', 'log_freq'] \n",
    "for col in drop_cols:\n",
    "    try:\n",
    "        df_models.drop(col, axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "# df_models.rename(columns={'use_weighted_sampler': 'sampler'}, inplace=True)\n",
    "\n",
    "# sort columns by name\n",
    "df_models = df_models.reindex(sorted(df_models.columns), axis=1)\n",
    "\n",
    "#sort rows by val/loss values\n",
    "df_models = df_models.sort_values(by=['val/l1'], ascending=True)\n",
    "# round all floats to two decimals\n",
    "df_models = df_models.round(2)\n",
    "\n",
    "# optional condense\n",
    "condense = True\n",
    "condense_cols = ['use_weighted_sampler','train/huber', 'train/shift_huber', 'samples_seen', 'n_params', 'backbone', 'Runtime', 'batch_size', ]\n",
    "if condense:\n",
    "    df_condensed = df_models.copy()\n",
    "    for col in condense_cols:\n",
    "        try:\n",
    "            df_condensed.drop(col, axis=1, inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "df_report = df_condensed.copy()\n",
    "report_drops = [ 'val/l1_20', 'val/l1_25', 'val/l2', 'val/shift_l1', 'val/shift_l2', 'val/huber', 'val/l1_30', 'val/loss']\n",
    "for col in report_drops, :\n",
    "    try:\n",
    "        df_report.drop(col, axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df_models.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns with \"val\" in the name by removing \"val/\"\n",
    "for col in df_condensed.filter(regex=\"val/\"):\n",
    "    try:\n",
    "        df_condensed.rename(columns={col: col.replace(\"val/\", \"\")}, inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to rename {col}: {str(e)}\")\n",
    "df_condensed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show df\n",
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condensed.to_csv(r\"C:\\Users\\45609\\dev\\Global-Canopy-Height-Map\\model_result_table.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
